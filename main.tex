% !TeX root = main.tex
\documentclass[11pt,a4paper,UTF8]{ctexart}
\usepackage{quicklatex} % 引入自定义样式

\title{端侧文生多模态大模型文献综述}
\author{邱梓豪}
\date{\today}

\begin{document}



\maketitle
\tableofcontents
\newpage

\section{扩散模型基础}

在本文中，我们首先提供一个简洁但完善的扩散模型的基础知识介绍，它涵盖三种主要形式：去噪扩散概率模型（Denoising Diffusion Probabilistic Models，DDPMs）\cite{sohldickstein2015diffusion,ho2020denoising}、基于分数的生成模型（Score-based Generative Models，SGMs）\cite{song2019generative,song2020improved}以及随机微分方程（Stochastic Differential Equations，Score SDEs）\cite{song2020score}。这些方法的核心思想是通过逐渐增强的随机噪声对数据进行扰动（称为“扩散”过程），然后逐步去除噪声以生成新的数据样本。我们阐明了它们如何在相同的扩散原理下工作，并解释了这三种模型如何相互关联并可以相互转化。

我们首先定义扩散模型中的常用符号。设离散时间变量为$t\in\{0,1,\cdots,T\}$，连续时间变量为$t\in[0,T]$。设原始数据$\x_0\sim q(\x_0)$，扩散模型通过向原始数据中不断加入高斯噪声，生成数据轨迹$\{\x_0,\x_1,\cdots,\x_T\}$，最终产生的$\x_T$趋近于纯高斯噪声。为了控制高斯噪声的添加强度，通常采用$\beta_t$（离散场景）或$\beta(t)dt$（连续场景）进行噪声调度。

\subsection{去噪扩散概率模型 (DDPM)}

去噪扩散概率模型（DDPM）使用两个马尔可夫链：一条前向链扰动数据直到变成噪声，另一条反向链将噪声转换回数据。前向链通常是人工设计的，目的是将任意数据分布转化为一个简单的先验分布（例如标准高斯分布），而反向马尔可夫链则通过学习由深度神经网络参数化的转移核（transition kernel）来逆转前向链的过程。随后，生成新数据点的过程首先从先验分布中采样一个随机向量，然后通过反向马尔可夫链进行采样，其中每一步都基于上一步的结果，该采样方式也称为\emph{祖先采样}\cite{koller2009probabilistic}。

下面我们给出上述过程的数学描述，给定数据分布$\x_0\sim q(\x_0)$，前向马尔可夫过程通过转移核$q(\x_t|\x_{t-1})$定义，并由此生成数据轨迹$\{\x_0,\x_1,\cdots,\x_T\}$。又马尔可夫性质，我们有：
\begin{equation*}
    q(\x_1,\cdots,\x_T|\x_0)=\Pi_{t=1}^T q(\x_t|\x_{t-1}).
\end{equation*}

在DDPM中，转移核$q(\x_t|\x_{t-1})$通常被设为高斯扰动形式，即：
\begin{equation*}
    q(\x_t|\x_{t-1}) =\N(\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t\mathbf{I}),
\end{equation*}
其中$\beta_t\in(0,1)$是用于噪声调度的超参数，并在模型训练前被设置好,$\mathbf{I}$是对角矩阵。根据Sohl-Dickstein等人\cite{sohldickstein2015diffusion}的结论，使用上面的高斯扰动转移核的好处在于我们可以方便地获得某时间步$t\in\{0,1,\cdots,T\}$的数据$\x_t$的边缘分布。具体地，我们有：
\begin{equation*}
    q(\x_t|\x_0)=\N(\x_t;\sqrt{\bar{\alpha_t}}\x_0,(1-\bar{\alpha_t})\mathbf{I}),
\end{equation*}
其中$\alpha_t:=1-\beta_t$，$\bar{\alpha_t}:=\Pi_{s=0}^t\alpha_s$。于是，给定$\x_0$，我们可以首先采样一个高斯向量$\epsilon\in\N(0,\mathbf{I})$，随后通过如下变换得到$\x_t$的采样：
\begin{equation*}
    \x_t=\sqrt{\bar{\alpha_t}}\x_0 + (1-\bar{\alpha_t})\epsilon.
\end{equation*}
不难看出，由于$\bar{\alpha_T}\approx 0$，所以$\x_T$近似于高斯分布，故有：$q(\x_T)=\int q(\x_T|\x_0)q(\x_0)d\x_0\approx \N(\x_T;\mathbf{0},\mathbf{I})$。

直观地说，前向过程会逐渐向数据中注入噪声，直到所有结构完全消失。为了生成新的数据样本，DDPMs首先从先验分布中生成一个无结构的噪声向量，然后通过运行一个可学习的马尔可夫链在时间反方向上逐步去除噪声。具体来说，反向马尔可夫链由先验分布$p(\x_T)=\N(\x_T;\mathbf{0},\mathbf{I})$和一个\emph{可学习}的转移核$p_{\theta}(\x_{t-1}|\x_t)$参数化。先验分布的形式被设定为$p(\x_T)=\N(\x_T;\mathbf{0},\mathbf{I})$是因为前向过程产生的$\x_T$满足$q(\x_T)\approx \N(\x_T;\mathbf{0},\mathbf{I})$。可学习的转移核$p_{\theta}(\x_{t-1}|\x_t)$的形式为：
\begin{equation*}
    p_{\theta}(\x_{t-1}|\x_t) = \N(\x_{t-1};\mu_{\theta}(\x_t,t),\Sigma_{\theta}(x_t,t)),
\end{equation*}
其中$\theta$表示模型参数，高斯均值$\mu_{\theta}(\x_t,t)$和方差$\Sigma_{\theta}(x_t,t))$函数由深度神经网络进行参数化，并以带噪声的数据样本$\x_t$和时间$t$为输入。假设我们可以学习好$p_{\theta}(\x_{t-1}|\x_t)$，那么便可以通过先从$p(\x_T)=\N(\x_T;\mathbf{0},\mathbf{I})$采样$\x_T$，再迭代式地调用$p_{\theta}(\x_{t-1}|\x_t)$（直到$t=1$）来生成样本$\x_0$。


\subsection{基于分数的生成模型 (SGM)}




\subsection{随机微分方程 (Score SDE)}


\newpage
\bibliography{ref}

\newpage
\appendix


\section{此处为附录}




\end{document}
