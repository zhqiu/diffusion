% !TeX root = main.tex
\documentclass[11pt,a4paper,UTF8]{ctexart}
\usepackage{quicklatex} % 引入自定义样式

\title{端侧文生多模态大模型文献综述}
\author{邱梓豪}
\date{\today}

\begin{document}



\maketitle
\tableofcontents
\newpage

\section{扩散模型数学基础}

本文首先系统阐述扩散模型的基础理论框架，涵盖三大主流范式：去噪扩散概率模型（Denoising Diffusion Probabilistic Model，DDPM）\cite{sohldickstein2015diffusion,ho2020denoising}、基于分数的生成模型（Score-based Generative Model，SGM）\cite{song2019generative,song2020improved}以及随机微分方程（Stochastic Differential Equation，SDE）框架下的分数生成模型（Score SDE）\cite{song2020score,karras2022elucidating}。这些方法均基于“先扰动后生成”的核心思想：通过渐进式噪声注入构建前向扩散过程，再通过反向去噪过程实现数据生成。本节将依次详细描述三类扩散模型的前向反向机制，阐明其内在关联与相互转化关系，最后综述基于该理论框架的重要改进工作。

我们首先定义用于描述扩散模型中的常用符号。设离散时间步为$t\in\{0,1,\cdots,T\}$，连续时间变量为$t\in[0,T]$。给定原始数据$\x_0\in\R^d$且$\x_0\sim q(\x_0)$，扩散模型通过向原始数据中不断加入高斯噪声构造数据轨迹$\{\x_0,\x_1,\cdots,\x_T\}$，最终使$\x_T$趋近于标准高斯分布。噪声注入强度由调度系数$\beta_t$（离散场景）或时变函数$\beta(t)dt$（连续场景）进行精确控制。

\subsection{去噪扩散概率模型 (DDPM)}

去噪扩散概率模型（Denoising Diffusion Probabilistic Model, DDPM）的核心框架基于两条相互关联的马尔可夫链：前向扩散链通过逐步添加噪声将原始数据分布扰动为简单的已知先验分布（如标准高斯分布），其中噪声调度可采用预设的固定策略；而反向生成链则通过可学习的参数化转移核（transition kernel）实现噪声到数据的渐进式重建，该转移核一般由深度神经网络进行参数化建模，其本质是通过训练来逼近前向扩散过程的逆过程。在生成阶段，模型首先从先验分布中抽取随机噪声向量，随后通过反向链执行多步\emph{祖先采样}（ancestral sampling）\cite{koller2009probabilistic}——即每一步的采样都严格依赖于前一步的隐变量状态，这种序列化采样机制最终将噪声逐步转化为符合目标数据分布的新样本。

\subsubsection{DDPM的前向与反向过程}

下面我们给出去噪扩散概率模型（DDPM）前向与反向过程的数学表述。设原始数据分布为$\x_0\sim q(\x_0)$，前向扩散过程定义为一个参数化的马尔可夫链，它通过转移核$q(\x_t|\x_{t-1})$生成隐变量序列（或称数据轨迹）$\{\x_0,\x_1,\cdots,\x_T\}$。根据马尔可夫性质，该过程的联合分布可分解为：
\begin{equation*}
    q(\x_1,\cdots,\x_T|\x_0)=\prod_{t=1}^T q(\x_t|\x_{t-1}).
\end{equation*}

在标准DDPM框架中，前向转移核$q(\x_t|\x_{t-1})$采用高斯扰动形式：
\begin{equation}
    q(\x_t|\x_{t-1}) =\N(\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t\mathbf{I}),
\label{ddpm:forward_process}
\end{equation}
其中$\beta_t\in(0,1)$是预先设定的噪声调度系数，$\mathbf{I}$是单位对角矩阵。正如Sohl-Dickstein等人\cite{sohldickstein2015diffusion}所述，使用上面设计的好处在于我们可以方便地获得某时间步$t\in\{0,1,\cdots,T\}$的数据$\x_t$的边缘分布。具体地，给定$\x_0$时$\x_t$的条件分布如下：
\begin{equation}
    q(\x_t|\x_0)=\N(\x_t;\sqrt{\bar{\alpha}_t}\x_0,(1-\bar{\alpha}_t)\mathbf{I}),
\label{ddpm:bar_alpha}
\end{equation}
其中$\alpha_t:=1-\beta_t$，$\bar{\alpha}_t:=\prod_{s=0}^t\alpha_s$。可以看出，上式实际上允许我们通过重参数化技巧（reparameterization trick）进行高效采样，即对于标准高斯噪声$\epsilon\in\N(0,\mathbf{I})$，$\x_t$的采样可表示为：
\begin{equation*}
    \x_t=\sqrt{\bar{\alpha_t}}\x_0 + (1-\bar{\alpha_t})\epsilon.
\end{equation*}
值得注意的是，当$T$足够大时，$\bar{\alpha}_T\approx 0$，使得边缘分布$q(\x_T)=\int q(\x_T|\x_0)q(\x_0)d\x_0$渐进收敛于标准高斯分布$\N(\mathbf{0},\mathbf{I})$，从而满足前向过程将数据完全扩散为噪声的设计目标。

从直观上看，前向扩散过程通过逐步注入噪声使数据结构性信息持续衰减，最终退化为无结构噪声。为了实现数据生成，DDPM首先从先验分布$p(\x_T)=\N(\x_T;\mathbf{0},\mathbf{I})$采样纯噪声（该设定的合理性源于前向过程保证$q(\x_T) \approx \mathcal{N}(\x_T; \mathbf{0}, \mathbf{I})$），随后通过可学习的反向马尔可夫链逐步去噪重构数据。该反向链的核心是参数化转移核：
\begin{equation*}
    p_{\theta}(\x_{t-1}|\x_t) = \N(\x_{t-1};\mu_{\theta}(\x_t,t),\Sigma_{\theta}(x_t,t)),
\end{equation*}
其中$\theta$表示模型参数，高斯均值$\mu_{\theta}(\x_t,t)$和方差$\Sigma_{\theta}(x_t,t))$函数由深度神经网络进行参数化，并以带噪声的数据样本$\x_t$和时间$t$为输入。假设我们可以充分学习$p_{\theta}(\x_{t-1}|\x_t)$，那么便可以通过先从$p(\x_T)=\N(\x_T;\mathbf{0},\mathbf{I})$采样$\x_T$，再迭代式地调用$p_{\theta}(\x_{t-1}|\x_t)$（直到$t=1$）来生成样本$\x_0$。完整的生成流程可表述为：
\begin{equation*}
\x_T \sim p(\x_T) \rightarrow \x_{T-1} \sim p_{\theta}(\x_{T-1}|\x_T) \rightarrow \cdots \rightarrow \x_0 \sim p_{\theta}(\x_0|\x_1).
\end{equation*}
我们将在下一节详细阐述如何通过数学推导和人为设计得到方便计算优化的$\mu_{\theta}(\x_t,t)$和$\Sigma_{\theta}(x_t,t))$。


\subsubsection{DDPM的训练与推理}

我们首先推导扩散模型的训练目标函数，即学习一个有效的反向条件分布$p_{\theta}(\x_{t-1}|\x_t)$。通过理论推导可以发现，该目标函数的具体形式将指导我们设计$p_{\theta}(\x_{t-1}|\x_t)$的参数化方式——具体而言，我们可以据此构造易于优化计算的均值函数$\mu_{\theta}(\x_t,t)$和方差函数$\Sigma_{\theta}(\x_t,t)$。从概率建模的角度，训练过程的核心在于通过最大化数据对数似然的变分下界（Evidence Lower Bound, ELBO）来实现模型优化。具体而言，给定数据样本$\x_0\sim q(\x_0)$，反向过程生成$\x_0$的对数似然为$\log p_{\theta}(\x_0)$，其变分下界推导如下：
\begin{align}
\begin{split}
    \log p_{\theta}(\x_0) &= \log \int p_{\theta}(\x_0,\cdots,\x_T)d\x_{1:T} \\
    &= \log \int p_{\theta}(\x_0,\cdots,\x_T)\frac{q(\x_1,\cdots,\x_T|\x_0)}{q(\x_1,\cdots,\x_T|\x_0)}d\x_{1:T} \\
    &= \log \int q(\x_1,\cdots,\x_T|\x_0)\frac{p_{\theta}(\x_0,\cdots,\x_T)}{q(\x_1,\cdots,\x_T|\x_0)}d\x_{1:T} \\
    &= \log \E_q \left[\frac{p_{\theta}(\x_0,\cdots,\x_T)}{q(\x_1,\cdots,\x_T|\x_0)}\right] \\
    & \geq \E_q \left[\log \frac{p_{\theta}(\x_0,\cdots,\x_T)}{q(\x_1,\cdots,\x_T|\x_0)}\right].
\end{split}
\label{ddpm:1}
\end{align}
上面最后一个不等式基于的Jensen不等式。此外，由前向和反向过程的马尔可夫性质，即
\begin{align*}
    p_{\theta}(\x_0,\cdots,\x_T) &= p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)\prod_{t=2}^T p_{\theta}(\x_{t-1}|\x_t) \\
    q(\x_1,\cdots,\x_T|\x_0) &= q(\x_T|\x_{T-1})\prod_{t=1}^{T-1}q(\x_t|\x_{t-1}),
\end{align*}
我们可以进一步对(\ref{ddpm:1})进行展开，得到：
\begin{align}
\begin{split}
    &\log p_{\theta}(\x_0) \geq \E_q \left[\log \frac{p_{\theta}(\x_0,\cdots,\x_T)}{q(\x_1,\cdots,\x_T|\x_0)}\right] \\
    &= \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)\prod_{t=2}^T p_{\theta}(\x_{t-1}|\x_t)}{q(\x_T|\x_{T-1})\prod_{t=1}^{T-1}q(\x_t|\x_{t-1})} \right] \\
    &= \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)\prod_{t=1}^{T-1} p_{\theta}(\x_{t}|\x_{t+1})}{q(\x_T|\x_{T-1})\prod_{t=1}^{T-1}q(\x_t|\x_{t-1})} \right] \\
    &= \E_q\left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)}{q(\x_T|\x_{T-1})} \right] + \E_q\left[\log\prod_{t=1}^{T-1}\frac{p_{\theta}(\x_{t}|\x_{t+1})}{q(\x_t|\x_{t-1})} \right] \\
    &= \E_q[\log p_{\theta}(\x_0|\x_1)] - \E_q[\text{KL}(q(\x_T|\x_{T-1}) \| p_{\theta}(\x_T))] -\Sigma_{t=1}^{T-1}\E_q[\text{KL}(q(\x_t|\x_{t-1}) \| p_{\theta}(\x_{t}|\x_{t+1}))],
\end{split}
\label{ddpm:2}
\end{align}
上面最后一个等式来自于KL散度的定义。此外，由于$p_{\theta}(\x_T)$的计算不需要扩散模型参与，实际上没有可学习参数，所以可以视为一个常数const。最终，我们可以得到如下简化的变分下界：
\begin{equation}
\log p_{\theta}(\x_0) \geq \E_q[\log p_{\theta}(\x_0|\x_1)]  -\Sigma_{t=1}^{T-1}\E_q[\text{KL}(q(\x_t|\x_{t-1}) \| p_{\theta}(\x_{t}|\x_{t+1}))] + \text{const}
\label{ddpm:3}
\end{equation}

当我们仔细观察上面的$\E_q[\text{KL}(q(\x_t|\x_{t-1}) \| p_{\theta}(\x_{t}|\x_{t+1}))]$项时，会发现该期望计算非常困难。具体来说，我们需要从分布$q(\x_{t-1},\x_{t+1}|\x_0)$中采样$\x_{t-1}$和$\x_{t+1}$来计算期望，而该分布的具体形式未知。为了解决这个问题，我们可以使用若干由$q$函数定义的前向转移函数\emph{模拟出}一个\emph{反向转移函数}来替代$q(\x_t|\x_{t-1})$，由于这个新的反向转移函数具有和$p_{\theta}(\x_t|\x_{t+1})$相同的方向，所以期望的计算可以大大简化。具体地，我们由贝叶斯定理，可得：
\begin{equation*}
    q(\x_t|\x_{t-1})=\frac{q(\x_{t-1}|\x_t)q(\x_t)}{q(\x_{t-1})},
\end{equation*}
又因为$\x_{t-1}$实际上是前向过程中由$\x_0$产生的，所以$\x_{t-1}$的分布依赖于$\x_0$，故我们将上式替换成下面的基于$\x_0$的形式：
\begin{equation}
    q(\x_t|\x_{t-1},\x_0)=\frac{q(\x_{t-1}|\x_t,\x_0)q(\x_t|\x_0)}{q(\x_{t-1}|\x_0)}.
\label{ddpm:bayes}
\end{equation}
可以看出上式中的$q(\x_{t-1}|\x_t,\x_0)$实际上就是我们想要的\emph{反向转移函数}。此时，我们可以根据(\ref{ddpm:1})推导出$\log p_{\theta}(\x_0)$的新变分下界如下：
\begin{align}
\begin{split}
    \log p_{\theta}(\x_0) & \geq \E_q \left[\log \frac{p_{\theta}(\x_0,\cdots,\x_T)}{q(\x_1,\cdots,\x_T|\x_0)}\right] \\
    &= \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)\prod_{t=2}^T p_{\theta}(\x_{t-1}|\x_t)}{q(\x_1|\x_0)\prod_{t=2}^T q(\x_t|\x_{t-1},\x_0)} \right] \\
    &= \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)}{q(\x_1|\x_0)} \right] + \E_q \left[\log \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_t|\x_{t-1},\x_0)} \right].
\end{split}
\label{ddpm:3}
\end{align}

随后，我们将(\ref{ddpm:bayes})代入(\ref{ddpm:3})的第二项，可得：
\begin{align}
\begin{split}
    \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_t|\x_{t-1},\x_0)} &= \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{\frac{q(\x_{t-1}|\x_t,\x_0)q(\x_t|\x_0)}{q(\x_{t-1}|\x_0)}} \\
    &= \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_{t-1}|\x_t,\x_0)}  \times \prod_{t=2}^T \frac{q(\x_{t-1}|\x_0)}{q(\x_t|\x_0)} \\
    &= \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_{t-1}|\x_t,\x_0)}  \times \frac{q(\x_1|\x_0)}{q(\x_T|\x_0)},
\end{split}
\label{ddpm:4}
\end{align}
其中最后一个等式成立是因为对任意序列$a_1,a_2,\cdots,a_T$，有$\prod_{t=2}^T\frac{a_{t-1}}{a_t}=\frac{a_1}{a_2}\times\frac{a_2}{a_3}\times\cdots\times\frac{a_{T-1}}{a_T}=\frac{a_1}{a_T}$。随后我们将(\ref{ddpm:4})代入(\ref{ddpm:3})中，可得：
\begin{align}
\begin{split}
    & \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)}{q(\x_1|\x_0)} \right] + \E_q \left[\log \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_t|\x_{t-1},\x_0)} \right] \\
    &= \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)}{q(\x_1|\x_0)} \right] + \E_q \left[\log \left(\prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_{t-1}|\x_t,\x_0)}  \times \frac{q(\x_1|\x_0)}{q(\x_T|\x_0)}\right) \right] \\
    &=\E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)}{q(\x_1|\x_0)} + \log \frac{q(\x_1|\x_0)}{q(\x_T|\x_0)} \right] + \E_q \left[\log \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_{t-1}|\x_t,\x_0)}  \right] \\
    &= \E_q \left[\log\frac{p_{\theta}(\x_T)p_{\theta}(\x_0|\x_1)}{q(\x_T|\x_0)} \right] + \E_q \left[\log \prod_{t=2}^T \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_{t-1}|\x_t,\x_0)}  \right] \\
    &= \E_q [p_{\theta}(\x_0|\x_1)] + \E_q\left[\frac{p_{\theta}(\x_T)}{q(\x_T|\x_0)}\right] + \Sigma_{t=2}^T \E_q \log \frac{p_{\theta}(\x_{t-1}|\x_t)}{q(\x_{t-1}|\x_t,\x_0)} \\
    &= \E_q [p_{\theta}(\x_0|\x_1)] -\text{KL}(q(\x_T|\x_0)\| p_{\theta}(\x_T)) -  \Sigma_{t=2}^T \text{KL}(q(\x_{t-1}|\x_t,\x_0) \| p_{\theta}(\x_{t-1}|\x_t)),
\end{split}
\label{ddpm:5}
\end{align}
其中我们在最后一个等式中使用了KL散度的定义。由于上式中$p_{\theta}(\x_T)$不需要学习，所以$\text{KL}(q(\x_T|\x_0)\| p_{\theta}(\x_T))$可以视作常数，因此，我们可以得到下面的数据对数似然$\log p_{\theta}(\x_0)$的变分下界：
\begin{equation}
\log p_{\theta}(\x_0) \geq \E_q [\log p_{\theta}(\x_0|\x_1)] -  \Sigma_{t=2}^T \text{KL}(q(\x_{t-1}|\x_t,\x_0) \| p_{\theta}(\x_{t-1}|\x_t)) + \text{const}.
\label{ddpm:elbo}
\end{equation}
可以看出，计算(\ref{ddpm:elbo})中的KL项（本质上是期望）所需的$\x_t$和$\x_0$实际上可以从分布$q(\x_t|\x_0)$中采样得到，该分布具有简洁的形式，因此(\ref{ddpm:elbo})相比前面提到的(\ref{ddpm:3})计算上简单很多。于是我们可以定义\emph{变分下界损失}$\mathcal{L}_{\text{VLB}}$如下：
\begin{equation}
\mathcal{L}_{\text{VLB}} := -\E_q [\log p_{\theta}(\x_0|\x_1)] +  \Sigma_{t=2}^T \text{KL}(q(\x_{t-1}|\x_t,\x_0) \| p_{\theta}(\x_{t-1}|\x_t)),
\label{ddpm:loss_vlb}
\end{equation}
显然，最小化$\mathcal{L}_{\text{VLB}}$即是最大化(\ref{ddpm:elbo})中的变分下界，等价于最大化扩散模型生成数据的似然。


下一步我们要确定$q(\x_{t-1}|\x_t,\x_0)$的具体形式。由(\ref{ddpm:bayes})可知，$q(\x_{t-1}|\x_t,\x_0)$由$q(\x_{t}|\x_{t-1},\x_0)$、$q(\x_{t-1}|\x_0)$和$q(\x_{t}|\x_0)$决定，而由扩散模型前向过程的定义可知，这三项的表达式分别为：
\begin{align*}
    q(\x_{t}|\x_{t-1},\x_0) &= \N(\x_t|\sqrt{\alpha_t}\x_{t-1},(1-\alpha_t)\mathbf{I}),  \\
    q(\x_{t-1}|\x_0) &= \N(\x_{t-1}|\sqrt{\bar{\alpha}_{t-1}}\x_0,(1-\bar{\alpha}_{t-1})\mathbf{I}), \\
    q(\x_{t}|\x_0) &=\N(\x_{t}|\sqrt{\bar{\alpha}_{t}}\x_0,(1-\bar{\alpha}_{t})\mathbf{I}),
\end{align*}
所以，由(\ref{ddpm:bayes})，我们可以得到
\begin{equation}
    q(\x_{t-1}|\x_t,\x_0) = \frac{\N(\x_t|\sqrt{\alpha_t}\x_{t-1},(1-\alpha_t)\mathbf{I})\N(\x_{t-1}|\sqrt{\bar{\alpha}_{t-1}}\x_0,(1-\bar{\alpha}_{t-1})\mathbf{I})}{\N(\x_{t}|\sqrt{\bar{\alpha}_{t}}\x_0,(1-\bar{\alpha}_{t})\mathbf{I})}.
\label{ddpm:dist_of_reverse_pre}
\end{equation}

随后，我们实际上可以证明$q(\x_{t-1}|\x_t,\x_0)$也服从高斯分布，并具有$\N(\x_{t-1}|\A\x_t+\B\x_0,\C\mathbf{I})$的形式，为了确认$\A$，$\B$和$\C$的具体形式，我们可以首先将($\ref{ddpm:dist_of_reverse_pre}$)中右侧三个高斯分布的乘积转化为指数之和，随后可以通过求一次导和二次导得到$\A$，$\B$和$\C$。这里我们省略具体过程，$q(\x_{t-1}|\x_t,\x_0)$的最后形式如下：
\begin{align}
\begin{split}
    q(\x_{t-1}|\x_t,\x_0) &= \N(\x_{t-1}|\mu_q(\x_t,\x_0),\Sigma_q(t)),\\
    \mu_q(\x_t,\x_0) &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t}}{1-\bar{\alpha}_t}\x_t + \frac{(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_t}\x_0 \\
    \Sigma_q(t) &= \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{I}  \stackrel{\text{def}}{=} \sigma_q^2(t)\mathbf{I},
\end{split}
\label{ddpm:dist_of_reverse}
\end{align}
其中$\bar{\alpha}_t=\Pi_{i=1}^t \alpha_i$。

现在我们已经得到了变分下界(\ref{ddpm:elbo})中$q(\x_{t-1}|\x_t,\x_0)$所具有的高斯形式，由于$p_{\theta}(\x_{t-1}|\x_t)$是可以设计的，所以我们选择将其也设置为一个高斯分布，因为两个高斯分布的KL散度是便于简化计算的。具体地，我们定义：
\begin{equation}
p_{\theta}(\x_{t-1}|\x_t) = \N(\x_{t-1}| \mu_{\theta}(\x_t,t), \sigma_q^2(t)\mathbf{I}),
\label{ddpm:p_reverse}
\end{equation}
其中均值向量$\mu_{\theta}(\x_t,t)$是由一个神经网络生成的，其具体形式我们将在后文中给出；至于方差，我们则是选择了与(\ref{ddpm:dist_of_reverse})中$q(\x_{t-1}|\x_t,\x_0)$相同的$\sigma_q^2(t)\mathbf{I}$。于是(\ref{ddpm:elbo})中的KL散度可以简化为
\begin{align}
\begin{split}
    & \text{KL}\left(q(\x_{t-1}|\x_t,\x_0) \| p_{\theta}(\x_{t-1}|\x_t)\right) \\
    &= \text{KL} \left(\N(\x_{t-1}|\mu_q(\x_t,\x_0),\sigma_q^2(t)\mathbf{I}) \| \N(\x_{t-1}| \mu_{\theta}(\x_t,t), \sigma_q^2(t)\mathbf{I})\right) \\
    &= \frac{1}{2\sigma_q^2(t)} \| \mu_q(\x_t,\x_0) - \mu_{\theta}(\x_t,t) \|^2,
\label{ddpm:elbo_simp_1}
\end{split}
\end{align}
在上面的推导中，我们使用了两个方差相同的高斯分布的KL散度实际上是这两个分布均值向量的欧式距离这一关系。

此外，对于变分下界(\ref{ddpm:elbo})中的$\log p_{\theta}(\x_0|\x_1)$可以进行如下简化：
\begin{align}
\begin{split}
\log p_{\theta}(\x_0|\x_1) &= \log \N(\x_0| \mu_{\theta}(\x_1,1),\sigma_q^2(1)\mathbf{I}) \\
&= \log \left( \frac{1}{\left(\sqrt{2\pi\sigma_q^2(1)} \right)^d} \right)\exp \left\{-\frac{\|\x_0 -\mu_{\theta}(\x_1,1) \|^2}{2\sigma_q^2(1)} \right\} \\
&= -\frac{\|\x_0 -\mu_{\theta}(\x_1,1) \|^2}{2\sigma_q^2(1)} -\frac{d}{2}\log (2\pi\sigma_q^2(1)),
\label{ddpm:elbo_simp_2}
\end{split}
\end{align}
可以看出(\ref{ddpm:elbo_simp_1})和(\ref{ddpm:elbo_simp_2})从优化目标上实际上是统一的，都是\emph{最小化前向过程产生的数据轨迹和对应的反向过程生成的数据轨迹的欧式距离}。

下面，我们再对$\mu_q(\x_t,\x_0)$的具体形式进行分析，并由此设计$\mu_{\theta}(\x_t,t)$的计算方式，最后完成DDPM目标函数的推导。我们由$q(\x_t|\x_0)$的表达式可知：
\begin{equation*}
    \x_t =\sqrt{\bar{\alpha}_t}\x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon \Rightarrow  \x_0 =\frac{\x_t - \sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}.
\end{equation*}
随后我们将上式代入(\ref{ddpm:dist_of_reverse})中$\mu_q(\x_t,\x_0)$的表达式，可以得到：
\begin{align}
\begin{split}
    \mu_q(\x_t,\x_0) &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t}\x_t +(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}\x_0 }{1-\bar{\alpha}_t} \\
    &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t}\x_t +(1-\alpha_t)\sqrt{\bar{\alpha}_{t-1}}\frac{\x_t - \sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}} }{1-\bar{\alpha}_t}\\
    &= \frac{1}{\sqrt{\alpha_t}}\x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\epsilon,
\end{split}
\label{ddpm:mu_q}
\end{align}
上式实际上将$\mu_q(\x_t,\x_0)$由关于$\x_0$的函数转化成关于高斯噪声$\epsilon$的函数。受此启发，下面我们将设计一个更合适的$\mu_{\theta}(\x_t,t)$的形式，使之适配$\mu_q(\x_t,\x_0)$的表达式，具体地，我们进行如下设计：
\begin{equation}
\mu_{\theta}(\x_t,t)=\frac{1}{\sqrt{\alpha_t}}\x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\epsilon_{\theta}(\x_t,t),
\label{ddpm:mu_theta}
\end{equation}
值得注意的是，我们此时让模型预测第$t$步中向$\x_t$中加入的噪声，即$\epsilon_{\theta}(\x_t,t)$。

最后，我们将(\ref{ddpm:elbo_simp_1})、(\ref{ddpm:elbo_simp_2})、(\ref{ddpm:mu_q})和(\ref{ddpm:mu_theta})代入(\ref{ddpm:elbo})，并结合(\ref{ddpm:loss_vlb})的定义，可得：
\begin{equation}
\mathcal{L}_{\text{VLB}} = -\sum_{t=1}^T \E_q \left[\frac{1}{2\sigma_q^2(t)}\frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t}\|\epsilon_{\theta}(\x_t,t)-\epsilon \|^2 \right],
\label{ddpm:loss_vlb_denoise}
\end{equation}
于是我们最终得到了DDPM的目标函数，它实际上是一个监督学习型的目标函数，即训练扩散模型的输出，即$\epsilon(\x_t,t)$，来预测前向过程中每一步加入的噪声$\epsilon$。注意，实际上加入的噪声是$\epsilon$乘上某系数，但我们设计扩散模型反向过程中也让$\epsilon(\x_t,t)$乘上此系数，所以在(\ref{ddpm:loss_vlb_denoise})中该系数可以被提取出来，形成系数$\frac{1}{2\sigma_q^2(t)}\frac{(1-\alpha_t)^2}{(1-\bar{\alpha}_t)\alpha_t}$。

最后我们介绍扩散模型的生成过程。在训练结束后，我们可以获得神经网络$\epsilon_{\theta}(\x_t,t)$。生成新样本时，我们先采样一个高斯白噪声$\x_T\sim\N(\mathbf{0},\mathbf{I})$，随后便可以利用(\ref{ddpm:p_reverse})中建立的$\x_{t-1}$与$\x_t$的关系以及(\ref{ddpm:mu_theta})中$\mu_\theta$的定义，迭代式地生成$\x_{T-1},\cdots,\x_0$，具体的计算方式为：
\begin{equation*}
    \x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\x_t -\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon(\x_t,t) \right) + \sigma_q(t)\z,\quad \z\sim\N(\mathbf{0},\mathbf{I}).
\end{equation*}


\subsection{基于分数的生成模型 (SGM)}

本节将系统阐述基于分数的生成模型（Score-based Generative Models, SGM）的核心理论框架。首先，我们将从Stein分数函数（简称分数函数）的基本概念与数学定义入手，建立理论基础；随后，我们深入探讨分数函数匹配（Score Matching）这一主流学习范式的原理与实现方法；最后，重点介绍当前SGM领域的前沿方法——噪声条件分数网络（Noise Conditional Score Network, NCSN）。

\subsubsection{Stein分数函数}

基于分数的生成模型的核心在于利用Stein分数函数（简称分数函数）\cite{Hyvrinen2005EstimationON}这一关键工具。对于给定的样本概率密度函数$p(\x)$，其Stein分数函数定义为对数概率密度的梯度场：
\begin{equation*}
    \s_\theta(\x) := \nabla_{\x}\log p(\x).
\end{equation*}
需要特别强调的是，此定义与统计学中常见的Fisher分数$\nabla_\theta \log p_{\theta}(\x)$存在本质区别：Stein分数函数关注的是\emph{样本空间}上的梯度变化，而非模型参数空间$\theta$的梯度信息。这一向量场揭示了数据分布的内在几何特征，其方向指示了使数据出现概率增加最快的路径。

当我们获得分数函数$\nabla_{\x}\log p(\x)$后，便可以通过朗之万动力学（Langevin dynamics）来从目标分布$p(\x)$中采样。具体地，给定步长$\alpha>0$以及一个初始值$\tilde{\x}_0\sim\pi(\x)$（$\pi(\x)$为任意先验分布），采样过程通过以下迭代公式完成：
\begin{equation*}
    \tilde{\x}_t = \tilde{\x}_{t-1} + \frac{\alpha}{2} \nabla_{\x}\log p(\tilde{\x}_{t-1}) + \sqrt{\alpha}\epsilon_t,\quad \epsilon_t\sim\N(\mathbf{0},\mathbf{I}).
\end{equation*}
当某些正则性条件被满足时，若$\alpha\rightarrow 0$且$T\rightarrow \infty$，\cite{max2011bayesian}的理论分析说明$\tilde{\x}_T$的分布将精确收敛于$p(\x)$。当$\alpha> 0$且$T< \infty$时上面的公式存在一些误差，需要用Metropolis-Hastings更新来修正，但在实践中通常可以忽略这种修正\cite{du2019implicit,nijkamp2019anatomy}。本文的后续分析将基于这一合理的工程假设展开。

上述理论框架揭示了基于分数的生成模型的核心优势：通过建立分数函数估计与朗之万动力学采样之间的数学联系，实现了从数据分布中直接采样的可能性。这种方法的有效性主要依赖于两个关键因素——对目标分布分数函数的准确估计，以及朗之万动力学过程的合理配置。下面我们将围绕这两个核心要素展开深入探讨。

\subsubsection{分数函数匹配}

在分数函数学习方法的演进过程中，研究者们提出了多种技术路线。一种方法称为显式分数函数匹配（Explicit Score Matching，ESM）\cite{vincent2011connection}。假设我们有一个包含$M$个样本的数据集$\X=\{\x^{(1)},\cdots,\x^{(M)}\}$，那么我们可以通过\emph{核密度估计}来近似真实数据分布$p(\x)$：
\begin{equation*}
    q_h(\x)=\frac{1}{M}\sum_{m=1}^M \frac{1}{h} K\left(\frac{\x-\x^{(m)}}{h} \right),
\end{equation*}
其中$K(\cdot)$是核函数，$h$是核函数的超参数。随后显式分数函数匹配基于$q_h(\x)$来学习分数函数$\s_\theta(\x)$，目标函数如下：
\begin{equation*}
    \mathcal{L}_{\text{ESM}}:=\frac{1}{2}\E_{p(\x)} \|\s_\theta(\x)-\nabla_{\x}\log p(\x) \|^2 \approx \frac{1}{2}\E_{q_h(\x)} \| s_\theta(\x)-\nabla_{\x}\log q_h(\x)\|^2.
\end{equation*}
该方法的主要问题在于核密度是非参数化的、它对真实分布的近似能力很有限，尤其是当样本数量有限或数据处于高维空间时，核密度估计的效果会较差。

为了避免近似真实数据分布，\cite{Hyvrinen2005EstimationON}提出了隐式分数函数匹配（Implicit Score Matching，ISM），其目标函数为：
\begin{equation*}
    \mathcal{L}_{\text{ISM}}:=\E_{p(\x)} \left[\text{Tr}(\nabla_{\x} \s_\theta (\x)) + \frac{1}{2}\|\s_\theta (\x) \|^2 \right],
\end{equation*}
其中$\nabla_{\x} \s_\theta (\x)$表示$\s_\theta (\x)$的Jacobian。尽管该方法避免了直接近似真实数据分布$p(\x)$，但\cite{song2019generative}指出该目标函数的迹函数难以计算，导致该方法缺乏可扩展性。

现在最流行的分数函数学习方法是基于去噪分数函数匹配（Denoising Score Matching，DSM）\cite{vincent2011connection}。给定一个样本$\x_0$，该方法使用一个预定义的噪声分布$q_\sigma(\x^\prime|\x)$对其进行扰动得到带噪声数据，其分布为$q_\sigma(\x^\prime):=\int q_\sigma(\x^\prime|\x)p(\x)d\x$。DSM的核心思想是使用分数函数匹配来估计\emph{该带噪声数据分布$q_\sigma(\x^\prime)$的分数函数}，优化目标如下：
\begin{equation}
\E_{q_\sigma(\x^\prime|\x)p(\x)} \left[\frac{1}{2}\| \s_{\theta}(\x^\prime) - \nabla_{\x^\prime}\log q(\x^\prime|\x) \|^2 \right].
\label{sgm:dsm}
\end{equation}
\cite{vincent2011connection}的结论显示最小化以上目标函数得到的最优分数函数（记为$\s_{\theta^*}(\x)$）\emph{几乎必然满足}$\s_{\theta^*}(\x)=\nabla_\x\log q_\sigma(\x)$。值得注意的是，只有当噪声足够小时，才有$q_\sigma(\x)\approx p(\x)$成立。

为了高效地计算(\ref{sgm:dsm})，一般可以设置$q(\x^\prime|\x)=\N(\x^\prime|\x,\sigma^2)$，即让$\x^\prime=\x+\sigma\epsilon,\epsilon\sim\N(\mathbf{0},\mathbf{I})$。此时我们有：
\begin{align*}
    \nabla_{\x^\prime}\log q(\x^\prime|\x) &= \nabla_{\x^\prime}\log \frac{1}{(\sqrt{2\pi\sigma^2})^d}\exp\left\{-\frac{\|\x^\prime-\x \|^2}{2\sigma^2}\right\} \\
    &= \nabla_{\x^\prime} \left\{ -\frac{\|\x^\prime-\x \|^2}{2\sigma^2} -\frac{d}{2}\log (2\pi\sigma^2) \right\} = -\frac{\x^\prime-\x}{\sigma^2}=-\frac{\epsilon}{\sigma}.
\end{align*}

于是(\ref{sgm:dsm})中的目标函数变成：
\begin{equation}
\E_{q_\sigma(\x^\prime|\x)p(\x)} \left[\frac{1}{2}\| \s_{\theta}(\x^\prime) - \nabla_{\x^\prime}\log q(\x^\prime|\x)  \|^2 \right] = \E_{q(\x^\prime)} \left[ \frac{1}{2}\left\| \s_\theta(\x +\sigma\epsilon)+\frac{\epsilon}{\sigma} \right\|^2\right].
\label{sgm:dsm_obj}
\end{equation}
可以看出，上面的目标函数的作用，就是训练分数函数$\s_\theta$，让其预测输入样本中的噪声（乘上系数$-1/\sigma$）。从物理视角理解，\emph{去除样本中的噪声本质上引导样本向数据分布的高概率区域移动}，这正是分数函数的定义。

\subsubsection{噪声条件分数网络}

尽管目标函数(\ref{sgm:dsm_obj})形式简洁且易于优化，但\cite{song2019generative}指出，由于真实世界数据常呈现低维流形分布，这会导致两个关键问题：其一，分数函数在一些区域可能无定义；其二，低密度区域的样本稀疏使得分数函数估计不准确。为解决这些问题，\cite{song2019generative}提出了噪声条件分数网络（Noise Conditional Score Network，NCSN）方法，其核心思想是通过多尺度高斯噪声扰动数据。具体而言，设$\sigma_1 > \sigma_2 > \cdots > \sigma_i > \cdots > \sigma_L > 0$是递减的$L$个不同的噪声尺度，通过以下方式构造带噪样本：
\begin{equation}
    q(\x^\prime|\x_0)=\N(\x^\prime|\x_0,\sigma_i^2\mathbf{I}),
\label{sgm:ncsn_forward}
\end{equation}
即$\x^\prime=\x_0+\sigma_i\epsilon$。此时的目标变成：
\begin{align}
\begin{split}
    \mathcal{L}_{\text{NCSN}}=\E_{i\in\{1,\cdots,L\},q(\x^\prime|\x_0)p(\x_0)} \left[\frac{1}{2} \lambda(\sigma_i)\left\| \s_\theta(\x^\prime,\sigma_i) + \frac{\epsilon}{\sigma_i} \right\|^2 \right],
\end{split}
\label{sgm:ncsn}
\end{align}
其中$\lambda(\sigma_i)$是一个取决于$\sigma_i$的权重系数，其目的是为了让不同损失项的量级不因$\sigma_i$的变化而变化，\cite{song2019generative}建议取$\lambda(\sigma)=\sigma^2$。这里的分数函数$\s_\theta(\x^\prime,\sigma_i)$同时接收带噪样本和噪声尺度作为输入。该方法具有双重优势：一方面，噪声扰动使数据脱离低维流形，确保分数函数在全空间有定义；另一方面，大尺度噪声能有效填充低密度区域，为分数估计提供更丰富的训练信号。

在$\s_\theta(\x,\sigma)$训练完成后，NCSN提出使用退火朗之万动力学（Annealed Langevin dynamics）来采样生成样本，其过程如下：
\begin{align}
\begin{split}
\alpha_i &\leftarrow \frac{\sigma_i^2}{\sigma_L^2}\alpha, \\
\x^\prime_t & \leftarrow\x^\prime_{t-1} + \frac{\alpha_i}{2}\s_{\theta}(\x^\prime_{t-1},\sigma_i) + \sqrt{\alpha_i}\epsilon,\ \epsilon\sim\N(\mathbf{0},\mathbf{I}),
\end{split}
\label{sgm:annealed_langevin}
\end{align}
该方法依次从大到小遍历所有噪声尺度：首先根据(\ref{sgm:annealed_langevin})第一行设置与$\sigma_i^2$成正比的步长$\alpha_i$，然后通过(\ref{sgm:annealed_langevin})第二行进行多步采样。与传统朗之万动力学相比，这种退火策略通过逐步降低噪声尺度，能够有效引导采样过程跨越分布间的低密度区域，从而生成更高质量的样本\cite{song2019generative}。

值得注意的是，对比DDPM的目标函数(\ref{ddpm:loss_vlb_denoise})与SGM的目标函数(\ref{sgm:ncsn})可以发现，DDPM中的$\epsilon_{\theta}(\x,t)$和SGM中的$\s_\theta(\x,\sigma)$在功能上是一致的，仅相差一个比例系数。我们将在下一节从理论角度深入分析这两类方法的内在联系。


\subsection{随机微分方程 (Score SDE)}
\label{subsection:sde}

我们此前讨论的DDPM和SGM方法均采用离散时间框架下的迭代关系来描述相邻时刻变量间的转移过程。在本节我们将看到，通过适当的连续化技术及数学变换，这些方法均可与连续时间框架下的微分方程建立深刻联系。我们将首先介绍微分方程（包括常微分方程与随机微分方程）的基本概念，继而给出DDPM和SGM对应的随机微分方程表示。

\subsubsection{常微分方程与随机微分方程}

我们首先介绍常微分方程（Ordinary Differential Equation，ODE）和随机微分方程（Stochastic Differential Equation，SDE）的概念。我们先介绍ODE，一阶（first-order）ODE的数学表示如下：
\begin{equation*}
    \frac{d\x(t)}{dt}=\f(t,\x),
\end{equation*}
其中$\f(t,\x)$是关于$t$和$\x$的确定性函数。在后续分析中，我们将主要采用其微分形式（differential form）：
\begin{equation*}
    d\x=\f(t,\x)dt.
\end{equation*}
该形式更直观地反映了状态变量$\x$在无穷小时间间隔$dt$内的演化规律。

SDE在ODE基础上引入了\emph{随机扰动项}，其一般形式为：
\begin{equation*}
    \frac{d\x(t)}{dt}=\f(t,\x) + \mathbf{g}(t,\x)\xi(t),\ \text{where}\ \xi(t)\sim\N(\mathbf{0},\mathbf{I}),
\end{equation*}
其中$\xi(t)$表示标准高斯白噪声过程，刻画了时间$t$的随机性。SDE对应的微分形式可以写成：
\begin{equation*}
    d\x=\f(t,\x)dt + \mathbf{g}(t,\x)d\w,
\end{equation*}
上面的$d\w=\xi(t)dt$是布朗运动（Brownian motion，又称Wiener过程）的微分形式，其满足：
\begin{equation*}
\E[d\w] = \mathbf{0}, \quad \text{Cov}(d\w) = \mathbf{I}dt.
\end{equation*}
与确定性ODE不同，SDE中的$\mathbf{g}(t,\x)d\w$项刻画了随机扰动对系统演化的影响强度。

\subsubsection{微分方程视角下的扩散模型}

在介绍SDE视角下DDPM和SGM的前向与反向过程前，我们首先分别定义通用的前向和反向过程的微分形式SDE。具体地，刻画前向过程的微分形式SDE如下：
\begin{equation}
    d\x=\underbrace{\f(t,\x)}_{\text{漂移项}}dt + \underbrace{g(t)}_{\text{扩散项}}d\w,
\label{sde:forward}
\end{equation}
其中$\xi(t)$对于所有时刻$t$均为独立同分布的高斯随机变量，$d\w=\xi(t)dt$是布朗运动的增量，其中$\xi(t)$可以看成布朗运动在时刻$t$的变化率。从积分的角度，布朗运动$\w(t)$正是通过对这些随机增量$\xi(t)dt$积分得到的。函数$\f(t,\x)$和$g(t)$具有明确的物理含义，前者称为\emph{漂移项}，它定义了在封闭系统中，分子在没有随机效应时的确定性运动方式；后者称为\emph{扩散项}，它描述了分子如何通过布朗运动随机地从一个位置跃迁到另一个位置，该函数决定了随机运动的强度参数。

扩散模型的反向过程对应于前向SDE在时间上反向演化，根据Anderson\cite{anderson1982reverse}的结论，(\ref{sde:forward})的时间反演形式可表述为：
\begin{equation}
    d\x= [\underbrace{\f(t,\x)}_{\text{漂移项}} - g(t)^2 \underbrace{\nabla_\x \log p_t(\x)}_{\text{分数函数}} ]dt + \underbrace{g(t)d\bar{\w}}_{\text{反向时间扩散项}},
\label{sde:backward}
\end{equation}
其中$p_t(\x)$是$t$时刻$\x$的概率分布，$\bar{\w}$是时间反向演化时的维纳过程（Wiener process）。

Song等人\cite{song2019generative}创造性地建立了概率流ODE（probability flow ODE），其形式如下：
\begin{equation}
    d\x= \left[\f(t,\x) -\frac{1}{2} g(t)^2\nabla_\x \log p_t(\x)\right]dt.
\label{ode:backward}
\end{equation}
该确定性微分方程与反向时间SDE具有完全相同的边际分布特性。这一深刻联系意味着，概率流ODE和反向时间SDE均可作为从目标分布采样的有效途径。接下来我们将具体分析DDPM和SGM框架中的前向和反向SDE形式。由于概率流ODE可以直接由反向时间SDE的系数$\f(t,\x)$，$g(t)$和$\nabla_\x\log p_t(\x)$定义，其推导过程在此不再赘述。

\subsubsection{DDPM的随机微分方程形式}

下面我们结合(\ref{sde:forward})和(\ref{sde:backward})，给出DDPM的前向和反向过程的SDE表达。首先由(\ref{ddpm:forward_process})中定义的DDPM前向过程的转移核$q(\x_t|\x_{t-1})$可知：
\begin{equation*}
    \x_i = \sqrt{1-\beta_i}\x_{i-1} + \sqrt{\beta_i}\z_{i-1},\ \z_{i-1}\sim\N(\mathbf{0},\mathbf{I}).
\end{equation*}

为了获得其对应的SDE表示，我们进行下面一系列设置。首先，我们将$\x_i,\x_{i-1},\z_i,\z_{i-1}$和$\beta_i$分别看成关于连续时间的函数，即
\begin{equation*}
    \x_i=\x(t+\Delta t),\ \x_{i-1}=\x(t),\ \z_i=\z(t+\Delta t),\ \z_{i-1}=\z(t),\ \beta_i = \beta(t+\Delta t)\Delta t,
\end{equation*}
其中$\Delta t$表示步长，$\beta(t)$为噪声调度函数。通过泰勒展开和极限分析，我们可以推导DDPM的前向过程的SDE如下：
\begin{align*}
\begin{split}
     \x_i &= \sqrt{1-\beta_i}\x_{i-1} + \sqrt{\beta_i}\z_{i-1} \\
    \Rightarrow\quad  \x(t+\Delta t) &= \sqrt{1-\beta(t+\Delta t)\Delta t}\ \x(t) + \sqrt{ \beta(t+\Delta t)\Delta t}\ \z(t) \\
    \Rightarrow\quad \x(t+\Delta t) &\approx \left(1- \frac{1}{2}\beta(t+\Delta t)\Delta t\right)\ \x(t) + \sqrt{ \beta(t+\Delta t)\Delta t}\ \z(t) \\
    \Rightarrow\quad \x(t+\Delta t) &\approx \x(t) - \frac{1}{2}\beta(t)\Delta t\ \x(t) + \sqrt{ \beta(t)\Delta t}\ \z(t),
\end{split}
\end{align*}
所以，当$\Delta t\rightarrow 0$时，我们便得到了DDPM前向过程的连续时间SDE的表达式：
\begin{equation}
    d\x = -\frac{1}{2}\beta(t)\x dt + \sqrt{\beta(t)}d\w.
\label{sde:ddpm_forward}
\end{equation}

对比(\ref{sde:ddpm_forward})和(\ref{sde:forward})，我们可以发现DDPM中，$\f(t,\x)=-\frac{1}{2}\beta(t)\x$，$g(t)=\sqrt{\beta_t}$，将这两个函数的表达式代入(\ref{sde:backward})，我们便可以得到DDPM反向过程的SDE表示：
\begin{align}
\begin{split}
    d\x &= \left[-\frac{1}{2}\beta(t)\x -\beta(t) \nabla_\x \log p_t(\x) \right] dt + \sqrt{\beta_t} d\bar{\w} \\
    &= -\beta(t) \left[\frac{\x}{2} + \nabla_\x \log p_t(\x) \right]dt + \sqrt{\beta_t} d\bar{\w}.
\end{split} 
\label{sde:ddpm_backward}
\end{align}

\subsubsection{SGM的随机微分方程形式}

我们接下来推导基于分数的生成模型（SGM）的前向和反向SDE表示，重点分析噪声条件评分网络（NCSN）方法\cite{song2020score}。NCSN通过多尺度噪声扰动数据，采用退火朗之万动力学进行反向生成。考虑$L$个递减的噪声尺度$\sigma_1 > \sigma_2 > \cdots > \sigma_L > 0$，NCSN的前向过程按照(\ref{sgm:ncsn_forward})在数据$\x_0$上逐步添加高斯噪声实现，即：
\begin{equation*}
    p(\x_i|\x_0) =\N(\x_i|\x_0,\sigma_i^2),\ p(\x_{i-1}|\x_0) =\N(\x_{i-1}|\x_0,\sigma_{i-1}^2).
\end{equation*}

下面我们建立$\x_i$与$\x_{i-1}$间的关系，首先这两个变量的均值相同，故它们之间的差值是一个均值为$\mathbf{0}$的高斯变量，记为$\u$，故有$\x_i=\x_{i-1}+\u$。此外，注意到$\x_{i-1}$和$\u$相互独立，故$\text{Var}[\x_i]=\text{Var}[\x_{i-1}]+\text{Var}[\u]$，所以有$\text{Var}[\u]=\text{Var}[\x_i]-\text{Var}[\x_{i-1}]=\sigma_i^2-\sigma_{i-1}^2$。综上所述，我们有：
\begin{equation*}
    \x_i = \x_{i-1} + \sqrt{\sigma_i^2-\sigma_{i-1}^2} \z_{i-1},\ i=1,2,\cdots,L.
\end{equation*}

随后我们根据上式来推导NCSN前向过程的连续时间SDE形式。针对$\x_i,\x_{i-1},\z_{i-1}$和$\sigma_i,\sigma_{i-1}$，我们使用与DDPM前向过程的SDE推导相同的连续函数形式，即
\begin{equation*}
    \x_i=\x(t+\Delta t),\ \x_{i-1}=\x(t),\ \z_{i-1}=\z(t),\ \sigma_i = \sigma(t+\Delta t),\ \sigma_{i-1} = \sigma(t),
\end{equation*}
其中$\Delta t$表示步长，$\sigma(t)$是噪声调度函数。通过泰勒展开和极限分析，我们有：
\begin{align*}
    \x_i &= \x_{i-1} + \sqrt{\sigma_i^2-\sigma_{i-1}^2}\ \z_{i-1} \\
    \Rightarrow\quad \x(t+\Delta t) &=\x(t) + \sqrt{\sigma(t+\Delta t) -\sigma(t)}\ \z(t) \\
    \Rightarrow\quad \x(t+\Delta t) &\approx \x(t) + \sqrt{\frac{d[\sigma(t)]^2}{dt}\Delta t}\ \z(t).
\end{align*}
所以，当$\Delta t\rightarrow 0$时，我们便得到了SGM前向过程的SDE的表达式：
\begin{equation}
    d\x = \sqrt{\frac{d[\sigma(t)]^2}{dt}}d\w.
\label{sde:sgm_forward}
\end{equation}
对比(\ref{sde:sgm_forward})和(\ref{sde:forward})，我们可以发现SGM中，$\f(t,\x)=0$，$g(t)=\sqrt{\frac{d[\sigma(t)]^2}{dt}}$，将这两个函数的表达式代入(\ref{sde:backward})，我们便可以得到SGM反向过程的SDE表示：
\begin{align}
\begin{split}
    d\x &= [\f(t,\x) - g(t)^2 \nabla_\x \log p_t(\x) ]dt + g(t)d\bar{\w} \\
    &= -\left(\frac{d[\sigma(t)]^2}{dt} \nabla_\x \log p_t(\x) \right)dt + \sqrt{\frac{d[\sigma(t)]^2}{dt}} d\bar{\w}.
\end{split} 
\label{sde:sgm_backward}
\end{align}

一旦我们能准确估计各时刻$t$的分数函数$\nabla_\x\log p_t(\x)$，便可以通过公式(\ref{sde:ddpm_backward})和(\ref{sde:sgm_backward})分别构建DDPM和SGM的反向过程SDE或概率流ODE。这些微分方程的解析解通常难以获得，因此在实际应用中主要依赖数值求解方法，例如退火朗之万动力学\cite{song2019generative}、数值SDE求解器\cite{jolicoeur2021gotta,song2020score}、数值ODE求解器\cite{karras2022elucidating,lu2022dpm,song2020denoising,song2020score}以及预测器-校正器（predictor-corrector）方法（结合马尔可夫链的解析解与数值ODE/SDE求解器）\cite{song2020score}，它们通过迭代方式逐步生成样本。关于这些技术的具体实现细节和比较分析，建议读者参阅相关文献。


\subsection{条件生成的核心方法}

在扩散模型中实现条件生成的核心挑战在于如何将外部信息（如文本、类别标签等）有效融入生成过程。我们这里用$y$表示外部信息，它可以通过一些网络结构（例如交叉注意力机制\cite{chen2021crossvit}等）注入扩散模型中，我们由此可以训练$p_{\theta}(\x_t|\x_{t+1},y)$来实现条件生成。但这种直接的生成方式往往效果不佳，主要有两个原因：首先，经典扩散损失函数主要约束生成样本与真实数据的分布匹配，但未显式优化条件$y$与生成内容$\x_0$的语义一致性；此外，$y$中某些信息（如物体位置、属性）需要在特定去噪步骤中动态调整生成方向，而固定结构的条件融合难以实现这种精细化控制\cite{saharia2022photorealistic}。

为克服上述挑战，研究者提出通过引入外部引导机制（guidance）强化条件信号对生成路径的调控能力，其中最具代表性的方法包括\textbf{分类器引导}（Classifier Guidance）、\textbf{无分类器引导}（Classifier-Free Guidance）和\textbf{CLIP引导}（CLIP Guidance）。本节首先形式化条件扩散模型的反向过程，随后系统阐述三类引导方法的核心机制与实现差异。

\subsubsection{条件扩散模型的反向过程}

根据\cite{dhariwal2021diffusion}的理论框架，我们从DDPM视角形式化\textbf{条件反向过程}（conditional reverse process）。我们之前介绍了无条件的$p_{\theta}(\x_{t}|\x_{t+1})$的定义，当引入外部信息$y$时，条件生成分布可通过贝叶斯定理推导如下：
\begin{align}
\begin{split}
    p(\x_{t}|\x_{t+1},y) &= \frac{p(\x_{t},\x_{t+1},y)}{p(\x_{t+1},y)} = \frac{p(\x_{t},\x_{t+1},y)}{p(y|\x_{t+1})p(\x_{t+1})} = \frac{p(\x_{t},y|\x_{t+1})p(\x_{t+1})}{p(y|\x_{t+1})p(\x_{t+1})} \\
    &= \frac{p(\x_{t}|\x_{t+1}) p(y|\x_{t},\x_{t+1})p(\x_{t+1})}{p(y|\x_{t+1})p(\x_{t+1})} = \frac{p(\x_{t}|\x_{t+1}) p(y|\x_{t},\x_{t+1})}{p(y|\x_{t+1})}.
\end{split}
\label{conditional:1}
\end{align}
又因为
\begin{equation*}
    p(y|\x_{t},\x_{t+1})=p(\x_{t+1}|\x_t,y)\frac{p(y|\x_t)}{p(\x_{t+1}|\x_t)} = p(\x_{t+1}|\x_t)\frac{p(y|\x_t)}{p(\x_{t+1}|\x_t)} = p(y|\x_t),
\end{equation*}
即由$\x_t$预测外部信息$y$实际上不依赖于$\x_{t+1}$。将上式代入(\ref{conditional:1})中，可得
\begin{equation*}
    p(\x_{t}|\x_{t+1},y) = \frac{p(\x_{t}|\x_{t+1})p(y|\x_t)}{p(y|\x_{t+1})}.
\end{equation*}
又因为$p(y|\x_{t+1})$与$\x_{t}$的生成无关，所以我们可以将此项视作一个常数$Z$。此外，上式中的$p(\x_{t}|\x_{t+1})$实际上可以用之前介绍的神经网络$p_\theta(\x_{t}|\x_{t+1})$近似，于是我们有：
\begin{equation}
    p_{\theta}(\x_{t}|\x_{t+1},y) = \frac{1}{Z} p_\theta(\x_{t}|\x_{t+1})p(y|\x_t).
\label{conditional:backward}  
\end{equation}

之前在介绍DDPM时，我们在(\ref{ddpm:p_reverse})中给出了$p_\theta(\x_{t}|\x_{t+1})$的具体形式，这里我们简记为$p_\theta(\x_{t}|\x_{t+1})=\N(\mu,\Sigma)$，下面我们参考\cite{dhariwal2021diffusion}，给出$p_{\theta}(\x_{t}|\x_{t+1},y)$对应的高斯分布的均值和方差表达式。首先，由$p_{\theta}(\x_{t}|\x_{t+1})$的定义，我们可以将其对数似然写成：
\begin{equation*}
    \log p_{\theta}(\x_{t}|\x_{t+1}) = -\frac{1}{2}(\x_t-\mu)^{T}\Sigma^{-1}(\x_t-\mu) + C_0,
\end{equation*}
其中$C_0$是一个与$\x_t$无关的常数。此外，对于(\ref{conditional:backward})中的$p(y|\x_t)$的对数似然，我们可以通过其在$\x_t=\mu$处进行一阶Taylor展开近似：
\begin{equation*}
    \log p(y|\x_t)\approx \log p(y|\x_t)|_{\x_t=\mu} + (\x_t-\mu)\nabla_{\x_t} \log p(y|\x_t)|_{\x_t=\mu} = (\x_t-\mu)g + C_1,
\end{equation*}
其中我们用$g$表示$\nabla_{\x_t} \log p(y|\x_t)|_{\x_t=\mu}$，$C_1$是一个与$\x_t$无关的常数。根据上面两式，我们可以推导(\ref{conditional:backward})中$p_{\theta}(\x_{t}|\x_{t+1},y)$的对数似然如下：
\begin{align}
\begin{split}
    \log p_{\theta}(\x_{t}|\x_{t+1},y) &= \log(p_\theta(\x_{t}|\x_{t+1})p(y|\x_t)) + C_2 \\
    &\approx -\frac{1}{2}(\x_t-\mu)^{T}\Sigma^{-1}(\x_t-\mu) + (\x_t-\mu)g + C_3 \\
    &= -\frac{1}{2} (\x_t-\mu-\Sigma g)^{T}\Sigma^{-1}(\x_t-\mu-\Sigma g) + \frac{1}{2}g^T\Sigma g + C_3 \\
    &= -\frac{1}{2} (\x_t-\mu-\Sigma g)^{T}\Sigma^{-1}(\x_t-\mu-\Sigma g) + C_4
\end{split}
\label{conditional:backward_gaussian}
\end{align}
其中$C_2$，$C_3$和$C_4$都是与$\x_t$无关的常数。从(\ref{conditional:backward_gaussian})中我们可以看出$p_{\theta}(\x_{t}|\x_{t+1},y)$实际上服从高斯分布$\N(\mu+\Sigma g,\Sigma)$。下面我们三种常用的条件生成方法，它们实际上分别采用不同的方式来实现$p(y|\x_t)$。

\subsubsection{分类器引导}

分类器引导（Classifier Guidance）\cite{dhariwal2021diffusion}通过训练一个分类器$p_{\phi}(y|\x_t)$（$\phi$表示模型参数）来建模条件概率$p(y|\x_t)$，并利用其梯度信息修正扩散路径。具体而言，在条件反向过程中，扩散模型的均值$\mu_\theta(\x_t|y)$需要按(\ref{conditional:backward_gaussian})进行偏移：
\begin{equation}
\hat{\mu}_{\theta}(\x_t|y)=\mu_{\theta}(\x_t|y) + s\cdot\Sigma_\theta(\x_t|y)\cdot\nabla_{\x_t}\log p_{\phi}(y|\x_t),
\label{conditional_classifier_guided}
\end{equation}
其中$s$是引导强度系数，$\Sigma_\theta(\x_t|y)$为预设的协方差矩阵。直观上，分类器引导通过$\nabla_{\x_t}\log p_{\phi}(y|\x_t)$项指示了增强条件相关性的更新方向。分类器引导虽然在实践中展示出来有效性，但是这种方法需要单独在噪声样本上训练分类器，这带来了额外的计算和存储开销，且在复杂条件（如长文本）下易受分类器精度限制。

\subsubsection{无分类器引导}

无分类器引导\cite{ho2022classifier}通过隐式地估计$p(y|\x_t)$来实现引导的目的，避免了显式分类器的训练需求。具体地，由贝叶斯定理，我们有：
\begin{equation*}
    p(y|\x_t) \propto \frac{p(\x_t|y)}{p(\x_t)},
\end{equation*}
从而推导出梯度关系：
\begin{equation*}
    \nabla_{\x_t}\log p(y|\x_t) \propto \nabla_{\x_t}\log \left(\frac{p(\x_t|y)}{p(\x_t)}\right) = \nabla_{\x_t}\log p(\x_t|y) -\nabla_{\x_t}\log p(\x_t).
\end{equation*}

在DDPM框架下，去噪网络$\epsilon_\theta(\x_t|y)$预测噪声隐式建模条件分数函数$\nabla_{\x_t}\log p(\x_t|y)$。类似地，$\epsilon_\theta(\x_t|\emptyset)$表示无条件分数函数$\nabla_{\x_t}\log p(\x_t)$，它可以通过在训练时以固定概率将条件$y$替换为空白标识符$\emptyset$实现。由此，条件梯度可进一步表示为：
\begin{align*}
    \nabla_{\x_t}\log p(y|\x_t) & \propto \nabla_{\x_t}\log p(\x_t|y) -\nabla_{\x_t}\log p(\x_t) \\
    &\propto \epsilon_\theta(\x_t|y) - \epsilon_\theta(\x_t|\emptyset),
\end{align*}
即有条件与无条件噪声预测的差值定义了条件修正方向。最终，无分类器引导的去噪函数为：
\begin{equation}
    \hat{\epsilon}_\theta(\x_t|y) = \epsilon_\theta(\x_t|\emptyset) + s\cdot(\epsilon_\theta(\x_t|y) - \epsilon_\theta(\x_t|\emptyset)),
\label{conditional_classifier_free_guided}
\end{equation}
其中$s$是控制引导强度的超参数。无分类器引导具有两个显著优势。首先，它允许单个模型在直接利用自身习得的知识进行生成，而无需依赖另一个分类模型。其次，当需要基于难以通过分类器预测的信息（例如文本）进行条件生成时，这种引导方式能够显著简化条件控制流程。

\subsubsection{CLIP引导}

CLIP（Contrastive Language-Image Pre-training）\cite{radford2021learning} 是一种基于大规模图像-文本对训练的跨模态表示学习框架，其核心目标是通过对比学习对齐视觉与语言语义空间。模型由图像编码器$f(x)$和文本编码器$g(y)$构成，通过优化InfoNCE对比损失\cite{oord2018representation}：
\begin{equation*}
\mathcal{L}_{\text{CLIP}} = -\log \frac{\exp(f(x_i)\cdot g(y_i)/\tau)}{\sum_{j=1}^N \exp(f(x_i)\cdot g(y_j)/\tau)},
\end{equation*}
其中$\tau$是温度系数，$N$为批大小。可以看出，对比学习能让匹配的图片-文本对$(x_i,y_i)$的特征相似度$f(x_i)\cdot g(y_i)$最大化，同时降低非匹配对的相似度

基于CLIP的语义对齐能力，研究者将其扩展至扩散模型的条件生成任务\cite{clip-guide-1,Patashnik_2021_ICCV,gal2022StyleGAN}。具体地，将CLIP的图文相似度$f(\x_t)\cdot g(y)$作为条件概率$p(y|\x_t)$的近似，并以此构造梯度引导项。根据式(\ref{conditional:backward_gaussian})，反向过程的均值修正为：
\begin{equation}
\hat{\mu}_{\theta}(\x_t|y)=\mu_{\theta}(\x_t|y) + s\cdot\Sigma_\theta(\x_t|y)\cdot\nabla_{\x_t}(f(\x_t)\cdot g(y)),
\label{conditional_clip_guided}
\end{equation}
其中$s$是控制引导强度的超参数，$\nabla_{\x_t}(f(\x_t)\cdot g(y))$表示图像特征对噪声样本$\x_t$的导数，用于驱动生成内容向文本描述$y$对齐。与分类器指导类似，必须对CLIP模型施加噪声训练（即在噪声图像$\x_t$上训练），以确保反向过程中梯度计算的正确性，这实际上限制了该方法的可扩展性。


\section{扩散模型后续改进}

后续关于扩散模型的研究主要从三个方向改进经典方法（DDPMs、SGMs和Score SDEs）：（1）更快、更高效的采样；（2）更精确的似然与密度估计；（3）处理常见的流形结构数据。我们将在接下来的分别对这些方向展开详细综述。

\subsection{更快更高效的采样}

扩散模型的采样加速技术可分为两类：无需额外训练的高效离散化方法（learning-free）和需要微调模型参数的训练优化方法（learning-based）。本文重点讨论基于数值求解随机微分方程（SDE）和常微分方程（ODE）的经典离散化方法，它们的核心在于对反向时间SDE或概率流ODE设计更高效的离散化方案，从而在保证生成质量的前提下显著降低采样步数。

\subsubsection{基于反向时间SDE的采样方法}

如\ref{subsection:sde}节所述，DDPM和SGM的反向过程均可视为反向时间SDE的离散化实现。Song等人\cite{song2019generative}开创性地提出了退火朗之万动力学（Annealed Langevin Dynamics, ALD），该方法通过双循环机制协调噪声衰减与采样过程：外循环逐步降低噪声尺度$\sigma_1>\sigma_2>\cdots>\sigma_L>0$，内循环在各噪声尺度下执行朗之万动力学采样。这种层级式设计使得采样初期能有效探索全局结构，后期则精细调整局部细节。Jolicoeur-Martineau等人\cite{JolicoeurMartineau2021AdversarialSM}指出ALD实际上存在噪声衰减不一致问题，即在第$i$轮外循环中，样本噪声尺度实际上大于理论期望的$\sigma_i^2$，这将导致朗之万动力无法完全收敛，噪声会不断累积。为此提出的改进方法一致退火采样（Consistent Annealed Sampling，CAS），通过动态调整分数函数与加入噪声的缩放系数，来确保每步的噪声方差严格匹配目标值$\sigma_i^2$（即一致性约束），避免误差传递。

Song等人\cite{song2020score}提出了一种与前向过程同步的方式来离散化反向时间SDE，即对前向时间SDE的每一步离散化，都有其对应的反向时间SDE离散化。\cite{song2020score}证明了这种反向时间SDE离散化实际上是(\ref{sde:backward})的数值SDE求解器。Jolicoeur-Martineau等人\cite{jolicoeur2021gotta}开发了一种具有自适应步长的SDE求解器，以加快生成速度。具体地，在每个时间步，该方法使用一个高阶SDE求解器与一个低阶SDE求解器生成新样本，如果两个样本越相似，那么说明这步的难度越低，那么步长就相应地设置得越大。

在物理动力学启发下，临界阻尼朗之万扩散（Critically-Damped Langevin Diffusion, CLD）\cite{dockhorn2021score}创新性地引入辅助速度变量构建增广状态空间。该方法设计的分数函数具有更平滑的扰动特性和更低的学习难度，配合定制的SDE积分器，在采样速度和质量上均取得显著提升。Song等人\cite{song2020score}提出的预测器-校正器框架则融合了数值SDE求解器（预测器）与马尔可夫链蒙特卡洛（MCMC）方法（校正器），该方法在每个时间步先用数值SDE求解器生成粗略样本，再利用MCMC校正器修正其边缘分布，确保样本在所有时间步与反向SDE解轨迹的分布一致。

\subsubsection{基于概率流ODE的采样方法}

相较于随机性的SDE求解，确定性ODE求解因消除随机扰动而具备更快的收敛速度，但可能牺牲部分生成多样性。去噪扩散隐式模型（Denoising Diffusion Implicit Model，DDIM）\cite{song2020denoising}是加速扩散模型采样的最早工作之一，它是对DDPM的改进，其核心思想是通过非马尔可夫（non-Markov）的扩散过程加速生成，同时保持与DDPM相同的目标函数。具体地，DDIM设计了如下非马尔可夫前向过程：
\begin{equation*}
    q(\x_1,\x_2,\cdots,\x_T|\x_0)=q(\x_T|\x_0)\Pi_{t=2}^T q(\x_{t-1}|\x_t,\x_0),
\end{equation*}
其中$q(\x_{t-1}|\x_t,\x_0)$为高斯分布，均值由$\x_t$和$\x_0$决定，它是非马尔可夫的。DDIM为了让前向过程的边缘分布与DDPM相同，即让$q(\x_t|\x_0)=\N(\sqrt{\alpha_t}\x_0,(1-\alpha_t)\mathbf{I})$并且$q(\x_{t-1}|\x_0)=\N(\sqrt{\alpha_{t-1}}\x_0,(1-\alpha_{t-1})\mathbf{I})$，推导出$q(\x_{t-1}|\x_t,\x_0)$的具体形式如下：
\begin{equation*}
    q(\x_{t-1}|\x_t,\x_0) = \N\left(\sqrt{\alpha_{t-1}}\x_0 + \sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\frac{\x_t-\sqrt{\alpha_t}\x_0}{\sqrt{1-\alpha_t}},\sigma_t^2\mathbf{I} \right).
\end{equation*}
DDIM与DDPM的训练目标一致，都是训练模型$\epsilon_\theta(\x_t,t)$去预测第$t$个时间步$\x_t$中的噪声。DDIM在反向过程生成新样本时利用了$q(\x_{t-1}|\x_t,\x_0)$，首先通过训练好的$\epsilon_\theta(\x_t,t)$来得到$\x_0$的估计：
\begin{equation*}
    \x_t = \sqrt{\alpha_t}\x_0 + \sqrt{1-\alpha_t}\epsilon\quad \Rightarrow\quad \hat{\x}_0 = \frac{\x_t - \sqrt{1-\alpha_t}\epsilon_\theta(\x_t,t)}{\sqrt{\alpha_t}},
\end{equation*}
随后将$\hat{\x}_0$代入非马尔可夫的$q(\x_{t-1}|\x_t,\x_0)$，并设$\sigma_t=0$，于是有：
\begin{equation*}
    \x_{t-1}=\sqrt{\alpha_{t-1}} \hat{\x}_0 + \sqrt{1-\alpha_{t-1}}\epsilon_\theta(\x_t,t).
\end{equation*}
值得注意的是，此时反向过程完全由$\x_t$和$\epsilon_\theta$决定，无随机噪声注入。从本质上来看，DDIM的反向过程对应于概率流ODE的一种特殊离散化方案，无噪声使得ODE的求解误差增长较慢，所以可以使用较大的步长，因而降低了采样步数。此外，由于DDIM的非马尔可夫设计，\cite{song2020denoising}证明了在部分步骤上反向过程的生成分布与完整过程的分布近似一致，故DDIM在采样时可以跳过一些冗余中间步骤，这进一步降低采样步数。

后续工作广义去噪扩散隐式模型（gDDIM）\cite{zhang2022gddim}提出通过分数函数网络的参数化方式，将DDIM推广到更一般的扩散模型中，例如模糊扩散模型（Blurring Diffusion Model，BDM）\cite{Hoogeboom2023blurring}和临界阻尼朗之万扩散（CLD）\cite{dockhorn2021score}等。PNDM\cite{liu2021pseudo}将扩散模型的逆向过程视为在$\mathbb{R}^N$中的特定流形上的微分方程求解问题，并提出一种伪数值方法使其在数据分布的高密度流形上生成样本，避免传统数值方法因偏离流形而导致的性能退化，DDIMs本质上是该方法的一种简单特例。


\subsubsection{基于高阶ODE求解器的采样方法}

针对概率流ODE的半线性结构（即包含一个线性项和神经网络非线性项）特点，最新研究开发了定制化高阶求解器。Karras等人\cite{karras2022elucidating}提出的二阶Heun方法\cite{ascher1998computer}通过额外的校正步骤减少局部截断误差，在样本质量和采样速度之间取得了较好的权衡。此外，他们还创新性地在ODE求解中引入随机朗之万扩散项（包含分数函数和随机噪声），以将样本推回数据流形以平衡质量与速度。

扩散指数积分器采样器\cite{zhang2022fast}和DPM-solver\cite{lu2022dpm}则充分利用ODE的线性-非线性分解特性，开发了专用ODE求解算法。相较于通用龙格-库塔方法，这些求解器对线性部分进行解析计算，并采用类指数积分器技术处理非线性项，其计算效率显著优于传统方法。实验表明，此类定制求解器仅需10-20次迭代即可生成高质量样本，而传统扩散模型通常需要数百次迭代才能达到同等效果，实现了数量级的速度提升。


\subsection{更精确的似然或密度估计}

扩散模型的训练目标本质上是数据对数似然的变分下界，然而该下界往往存在松弛性问题。研究表明，通过优化扩散过程中的噪声调度策略与反向过程的方差参数化方式，可以显著提升变分下界的紧致性。我们在本节将阐述几种该类型方法。

传统扩散模型的噪声调度通常采用启发式设计。iDDPM\cite{nichol2021improved}提出了一种基于余弦函数的噪声调度策略，显著提升了模型的对数似然性能。具体地，该工作将(\ref{ddpm:bar_alpha})中的$\bar{\alpha}_t$定义为：
\begin{equation*}
    \bar{\alpha}_t=\frac{f(t)}{f(0)},\ f(t)=\cos\left(\frac{t/T+s}{1+s}\cdot\frac{\pi}{2}\right)^2,
\end{equation*}
其中$s$是控制噪声尺度的超参数，$T$表示总扩散步数。对应的前向过程方差$\beta_t$可通过$\beta_t=1-\frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}$递推计算。此外，iDDPM还提出了一种基于对数域插值反向方差学习方法：
\begin{equation*}
    \Sigma_\theta(\x_t,t)=\exp (v\log\beta_t + (1-v)\log \sigma_q^2(t)),
\end{equation*}
其中$v$为扩散模型根据$\x_t$产生一个与$\beta_t$相同维度的向量，$\sigma_q^2(t)$由(\ref{ddpm:dist_of_reverse})定义。该方法通过可学习的方差参数化，有效提升了变分下界的紧致性。


变分扩散模型（Variational Diffusion Model，VDM）\cite{kingma2021variational}通过引入广义参数化形式扩展了传统扩散模型的理论框架。该模型将前向过程的边缘分布定义为$q(\x_t|\x_0)=\N(\x_t;\alpha_t\x_0,\sigma_t^2\mathbf{I})$，其中参数$\alpha_t$控制信号衰减强度，$\sigma_t^2$决定噪声注入量。值得注意的是，DDPM的经典形式(\ref{ddpm:bar_alpha})可视为VDM在特定参数约束下的特例——当约束条件$\alpha_t=\sqrt{1-\sigma_t^2}$成立时，此时信噪比$\alpha_t^2/\sigma_t^2$单调递减；而NCSN的前向过程(\ref{sgm:ncsn_forward})即为$\alpha_t=1$时的特殊形式。VDM进一步提出通过如下方式参数化$\sigma_t^2$：
\begin{equation*}
    \sigma_t^2=\text{sigmoid}(\gamma_\eta(t)),
\end{equation*}
其中$\gamma_\eta(t)$是参数为$\eta$的单调神经网络，它保证了$\x_t$的信噪比随时间单调递减的特性，还为学习复杂噪声调度提供了可微分的参数空间。

在逆向过程优化方面，Analytic-DPM\cite{bao2021analytic}从变分原理出发推导出最优反向方差的解析表达式：
\begin{equation*}
    \Sigma_\theta(\x_t,t)=\sigma_q^2(t) + \left(\sqrt{\frac{\bar{\beta}_t}{\alpha_t}} -\sqrt{\bar{\beta}_{t-1}-\sigma_q^2(t)} \right)^2\cdot \left(1-\bar{\beta}_t \E_{q(\x_t)}\frac{\|\nabla_{\x_t}\log q(\x_t) \|^2}{d} \right)
\end{equation*}
其中$\sigma_q^2(t)$由(\ref{ddpm:dist_of_reverse})定义，$\alpha_t=1-\beta_t$（$\beta_t$由(\ref{ddpm:forward_process})定义），
$\bar{\beta}_t=1-\bar{\alpha}_t$（$\bar{\alpha}_t$由(\ref{ddpm:bar_alpha})定义），$q(\x_t)$是前向过程生成的$\x_t$的边缘分布，$d$是数据的维度。该理论成果的核心价值在于给定预训练的分数模型后即可确定最优逆向方差，并且使用该最优方差可得到更紧致的变分下界估计。


\subsection{流形上的高效扩散模型学习}

传统的基于分数的生成模型（SGM）通常直接在数据空间（如像素空间）中应用，这导致采样时需要大量网络评估，计算成本高且效率低。潜在分数生成模型（Latent Score-Based Generative Model，LSGM）\cite{vahdat2021score}提出将SGM和变分自编码器（Variational Autoencoder，VAE）结合：通过VAE的编码器将高维数据映射至低维潜在空间，继而在此压缩空间进行SGM的训练和采样。该框架在保持原始SGM生成质量和概率分布覆盖能力的同时，显著提升了计算效率，有效平衡了模型性能与资源消耗。

区别于LSGM的联合训练范式，潜在扩散模型（Latent Diffusion Model，LDM）\cite{rombach2022high}采用分阶段优化策略。其技术核心在于：首先独立训练具有感知等效特性的分层VAE，构建高度语义压缩的潜在表示空间；随后在此稳定收敛的潜在空间中训练扩散概率模型。这种解耦式训练架构不仅降低了模型优化难度，更重要的是通过建立“感知等效-语义压缩”的潜在空间，使得扩散过程能聚焦于本质特征学习，从而在图像生成任务中实现计算复杂度降低与视觉保真度提升的双重突破。





\newpage
\bibliography{ref}

\newpage
\appendix


\section{此处为附录}




\end{document}
